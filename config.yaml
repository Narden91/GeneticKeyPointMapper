settings:
  verbose: 1 # 0: minimal, 1: basic EA output
  global_random_seed: 42

# --- NEW SECTION FOR POSE RETARGETING ---
pose_retargeting:
  data_base_path: "data/" # Base path to "TDB_XXX_Y" folders
  source_keypoints_file_suffix: "__KP3D.csv"
  target_keypoints_file_suffix: "__AL.csv"
  source_num_keypoints: 33
  target_num_keypoints: 54
  
  # For initial simplified run, focusing on one movement type
  # To use all, set movement_filter to null or an empty list
  # Example: movement_filter: ["JUMP", "WALK"]
  movement_filter: ["JUMP"] # e.g., "__F-JUMP__" pattern in filename
  
  # Subject-based train/test split
  # Option 1: Ratio (subjects will be randomly split)
  train_subject_ratio: 0.8 
  # Option 2: Explicit lists (if train_subject_ratio is null/not present)
  # train_subjects: ["TDB_001_F", "TDB_002_M"] 
  # test_subjects: ["TDB_003_F"]

  preprocessing:
    normalization_range: [0, 1] # [min, max] for coordinate scaling, or null to skip
    # outlier_filter: "median" # "savitzky_golay" or null (Not implemented in this simplified version)
    # missing_data_method: "linear_interpolate" # or "cubic_spline" (Not implemented in this simplified version)

genome_definition:
  C1_init_bounds: [0, 1]  # For correspondence matrix
  S_init_bounds: [0.5, 1.5] # For scaling factors
  B_init_bounds: [-0.2, 0.2] # For bias vectors

nsga3_optimizer:
  population_size: 50 # Start small
  num_generations: 30 # Start small
  crossover_prob: 0.9
  crossover_eta: 15 # For SBX
  mutation_prob: 0.1 # Per-variable mutation probability
  mutation_eta: 20  # For Polynomial Mutation

  # Weights for objectives (not used by NSGA-III directly, but for final selection or if simplifying to single objective later)
  # For now, these are conceptual for the multi-objective nature
  objective_weights:
    accuracy: 1.0
    temporal_consistency: 0.5
    # anatomical_plausibility: 0.2 # For future

model:
  type: LogisticRegression  # Options: RandomForest, XGBoost, CatBoost, LogisticRegression
  global_random_state: 42

  LogisticRegression_params:
    C: 1.0
    solver: 'lbfgs'  # For multiclass problems
    multi_class: 'multinomial'  # For multiclass problems
    max_iter: 1000
    # penalty: 'l2'  # default
    # class_weight: 'balanced'  # Optional for imbalanced classes

  RandomForest_params:
    n_estimators: 100
    max_depth: 5 # null means no limit or use scikit-learn default
    # min_samples_split: 2
    # min_samples_leaf: 1
    # random_state: 42 # Overrides global_random_state

  XGBoost_params:
    n_estimators: 100
    learning_rate: 0.1
    # max_depth: 3
    # subsample: 0.8
    # colsample_bytree: 0.8
    # objective: 'multi:softprob' # Auto-set: 'multi:softprob' for multiclass, 'binary:logistic' for binary
    # eval_metric: 'mlogloss'     # Auto-set: 'mlogloss' for multiclass, 'logloss' for binary
    # use_label_encoder: false    # Auto-set to False
    # random_state: 42            # Overrides global_random_state
    
  CatBoost_params:
    iterations: 200 # Equivalent to n_estimators
    learning_rate: 0.05
    # depth: 6
    # l2_leaf_reg: 3
    # loss_function: 'MultiClass' # Auto-set: 'MultiClass' for multiclass, 'Logloss' for binary
    # verbose: 0                  # Auto-set to 0 (silent) unless specified
    # random_seed: 42             # Overrides global_random_state (CatBoost uses random_seed)

cross_validation:
  n_splits: 5
  shuffle: True
  # random_state: 42 # Optional: uses model.global_random_state by default if not set here

calibration:
  enabled: False # Set to True to enable calibration
  default_method: "isotonic" # Fallback if specific model method not found
  randomforest_method: "isotonic" # or "sigmoid"
  xgboost_method: "isotonic"
  catboost_method: "isotonic" 

explainer:
  shap:
    enabled: False # Set to True to enable SHAP explanations
    summary_plot_path: "reports/shap_summary_plot.png" # Path to save SHAP summary plot
    # Add other SHAP specific parameters if your explainer.py uses them

bayesian_methods:
  params:
    alpha: 1.0
    beta: 1.0